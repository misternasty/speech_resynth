common:
  seed: 0

dataset:
  wav_dir: "data/LibriTTS_R_16k" # ${root}/train-clean-100, train-clean-360, ...
  wav_dir_orig: "data/LibriTTS_R"  # if wav_dir == wav_dir_orig, original wav files are overwritten with 16 kHz waveforms
  spectrogram_dir: "data/LibriTTS_R_16k/spectrogram"  # 34GB
  vad: false

  ext_audio: ".wav"
  ext_txt: ".normalized.txt"

  # json file format
  # "name": {"units": List[int], "durations": List[int], "transcript": str}
  train_file: "data/resynth/train.json"  # 354,729 samples
  dev_file: "data/resynth/dev.json"  # 5,736 samples
  test_file: "data/resynth/test.json"  # 9,957 samples

synthesis:
  src_dir: ${dataset.wav_dir}
  tgt_dir: ${dataset.wav_dir}_resynth
  split: "test-*"
  ext_audio: ${dataset.ext_audio}

eval:
  result_path: "results/resynth/score.csv"

flow_matching:
  path: "models/flow_matching"
  batch_size: 2700 # work with single 24GB VRAM GPU
  frames_per_seg: 100  # 2 seconds
  num_workers: 16
  epoch: 100
  warmup_steps: 1000
  lr: 0.001
  lr_min: 0.0001
  max_norm: 0.1
  summary_interval: 100
  save_interval_epoch: 20

  # inference
  dt: 0.0625
  truncation_value: 1.0  # truncation trick (https://arxiv.org/abs/1809.11096)

  # textless.data.speech_encoder.SpeechEncoder
  dense_model_name: "mhubert-base-vp_mls_cv_8lang"
  quantizer_model_name: "kmeans-expresso"
  vocab_size: 2000

  # src.flow_matching.configs.ConditionalFlowMatchingConfig
  dim_in: 80
  dim_cond_emb: 768
  hidden_size: 256
  depth: 4
  heads: 2
  intermediate_size: 896
  ff_dropout: 0.0
  use_unet_skip_connection: false
  conv_pos_embed_kernel_size: 31
  conv_pos_embed_groups: 256
  attn_dropout: 0.0
  mean: -5.8843  # mean of log mel-spectrogram
  std: 2.2615  # std of log mel-spectrogram
  predict_duration: false

hifigan:
  path: "models/fastspeech2_conformer_hifigan"
  batch_size: 64
  segment_size: 16080
  num_workers: 20
  training_epochs: 181  # 1M steps

  num_gpus: 1
  learning_rate: 0.0002
  adam_b1: 0.8
  adam_b2: 0.99
  lr_decay: 0.999
  seed: 1234

  upsample_rates: [5, 4, 4, 2, 2]
  upsample_kernel_sizes: [10, 9, 8, 4, 4]

  n_fft: 400  # HuBERT
  hop_size: 320  # HuBERT

  dist_config:
    dist_backend: "nccl"
    dist_url: "tcp://localhost:54321"
    world_size: 1

  stdout_interval: 1000
  summary_interval: 1000
  checkpoint_interval: 10000
  validation_interval: 10000

flow_matching_with_hifigan:
  name: "ryota-komatsu/flow_matching_with_hifigan"
  batch_size: 32

asr:
  name: "openai/whisper-large-v3"
